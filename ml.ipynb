{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbff17b-b954-42a3-a696-d768b9c0d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 20:50:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/20 21:09:00 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.ConnectException: Call From hadoop-01/10.100.30.57 to hadoop-03.uni.innopolis.ru:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\tat sun.reflect.GeneratedConstructorAccessor28.newInstance(Unknown Source)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n",
      "\tat com.sun.proxy.$Proxy28.getNewApplication(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:286)\n",
      "\tat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy29.getNewApplication(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:284)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:292)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:195)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n",
      "\t... 33 more\n",
      "25/05/20 21:09:01 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!\n",
      "25/05/20 21:09:01 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "25/05/20 21:09:01 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.ConnectException: Call From hadoop-01/10.100.30.57 to hadoop-03.uni.innopolis.ru:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.GeneratedConstructorAccessor28.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy28.getNewApplication(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:286)\n\tat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy29.getNewApplication(Unknown Source)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:284)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:292)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:195)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      4\u001b[39m team = \u001b[33m\"\u001b[39m\u001b[33mteam6\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m warehouse_location = \u001b[33m\"\u001b[39m\u001b[33mproject/hive/warehouse\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m spark = (\u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mteam6 – Interactive Analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myarn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# metastore URIs\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.hive.metastore.uris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthrift://hadoop-02.uni.innopolis.ru:9883\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# important: Hive database storage directory\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.hive.metastore.warehouse.dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/warehouse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.warehouse.dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/warehouse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m spark.conf.set(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.files.ignoreMissingFiles\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m spark.conf.set(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.files.ignoreCorruptFiles\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/context.py:203\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    201\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/context.py:296\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/context.py:421\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.ConnectException: Call From hadoop-01/10.100.30.57 to hadoop-03.uni.innopolis.ru:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.GeneratedConstructorAccessor28.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy28.getNewApplication(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:286)\n\tat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy29.getNewApplication(Unknown Source)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:284)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:292)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:195)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = \"team6\"\n",
    "warehouse_location = \"project/hive/warehouse\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"team6 – Interactive Analysis\")\n",
    "    .master(\"yarn\")\n",
    "    # metastore URIs\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\",\n",
    "            \"thrift://hadoop-02.uni.innopolis.ru:9883\")\n",
    "    # important: Hive database storage directory\n",
    "    .config(\"spark.hadoop.hive.metastore.warehouse.dir\",\n",
    "            \"hdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/warehouse\")\n",
    "    .config(\"spark.sql.warehouse.dir\",\n",
    "            \"hdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/warehouse\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\",  \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.hive.verifyPartitionPath\", \"false\")\n",
    "\n",
    "# Let's make sure the config is applied\n",
    "print(\"=> hive.metastore.uris:\", \n",
    "      spark.sparkContext.getConf().get(\"spark.hadoop.hive.metastore.uris\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395a83b-e82f-44be-97ce-1df3406934a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that Spark knows about your warehouse\n",
    "print(\"warehouse.dir =\",\n",
    "      spark.sparkContext.getConf()\n",
    "           .get(\"spark.hadoop.hive.metastore.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d839c6b-5b1d-42c9-8063-02c718b5524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = spark.catalog.listTables(\"team6_projectdb\")\n",
    "print(\"\\nTables in a team6_projectdb:\")\n",
    "for tbl in tables:\n",
    "    print(f\" - {tbl.name} (type={tbl.tableType})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953eb1d-4089-4072-a2fc-33258a46d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE team6_projectdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0680c-efe2-41e6-ae29-57ba399e9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emps = spark.table(\"team6_projectdb.fact_fhv_trips_part\")\n",
    "emps.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b5165-ba98-4263-a08a-53ed33f62582",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM team6_projectdb.dim_base\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44128520-d716-4302-b2c9-fec0559c472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Read Hive tables\n",
    "# 1) Читаем весь факт из HDFS, рекурсивно обходя папки month=*\n",
    "fhv_facts = spark.table(\"team6_projectdb.fact_fhv_trips_part\")\n",
    "base      = spark.table(\"team6_projectdb.dim_base\")\n",
    "calendar  = spark.table(\"team6_projectdb.dim_calendar\")\n",
    "location  = spark.table(\"team6_projectdb.dim_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c87a3-36a2-46b8-87a8-9d92c49c636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some queries\n",
    "fhv_facts.printSchema()\n",
    "base.printSchema()\n",
    "calendar.printSchema()\n",
    "location.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dbf2c-3d2e-4e8c-8252-0c527ee7a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins\n",
    "df_raw = (fhv_facts\n",
    "    .join(\n",
    "        base,\n",
    "        fhv_facts.dispatching_base_num == base.base_num,\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # 3) JOIN with fence zone information (pickup)\n",
    "    .join(\n",
    "        location.withColumnRenamed(\"location_id\", \"pu_location_id\"),\n",
    "        on=\"pu_location_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # 4) JOIN with dropoff zone information (dropoff)\n",
    "    .join(\n",
    "        location\n",
    "          .withColumnRenamed(\"location_id\", \"do_location_id\")\n",
    "          .withColumnRenamed(\"zone\",        \"do_zone\")\n",
    "          .withColumnRenamed(\"borough\",    \"do_borough\")\n",
    "          .withColumnRenamed(\"service_zone\",\"do_service_zone\"),\n",
    "        on=\"do_location_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d369b0e-8876-4b05-b2ee-f3d1680a1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    OneHotEncoder,\n",
    "    VectorAssembler,\n",
    "    StandardScaler\n",
    ")\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b3438-bfc3-41ec-94f3-49265ff1aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Convert UNIX timestamps to Timestamp and extract components\n",
    "df0 = (\n",
    "    df_raw\n",
    "      # конвертация в timestamp\n",
    "    .withColumn(\"req_ts\",   F.to_timestamp(\"request_datetime\"))\n",
    ")\n",
    "\n",
    "for prefix in [\"req\"]:\n",
    "    df0 = (\n",
    "        df0\n",
    "        .withColumn(f\"{prefix}_year\",  F.year (f\"{prefix}_ts\"))\n",
    "        .withColumn(f\"{prefix}_month\", F.month(f\"{prefix}_ts\"))\n",
    "        .withColumn(f\"{prefix}_day\",   F.dayofmonth (f\"{prefix}_ts\"))\n",
    "        .withColumn(f\"{prefix}_hour\",  F.hour (f\"{prefix}_ts\"))\n",
    "        .withColumn(f\"{prefix}_min\",   F.minute (f\"{prefix}_ts\"))\n",
    "        .withColumn(f\"{prefix}_sec\",   F.second (f\"{prefix}_ts\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a2e3d-3afe-4776-a175-e925e8865106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fda496-f394-4dd5-ad01-3af5cf7924a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = (\n",
    "    df0\n",
    "    # извлекаем номер дня недели 1..7\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"req_ts\"))\n",
    "    # извлекаем квартал 1..4\n",
    "    .withColumn(\"quarter\",    F.quarter(\"req_ts\"))\n",
    "    # булевый признак «выходной»\n",
    "    .withColumn(\"is_weekend_int\",\n",
    "                F.when(F.col(\"day_of_week\").isin(1,7), 1.0)\n",
    "                 .otherwise(0.0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c09e57-d551-4dcb-b432-ca7072869b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1479cf2-eaff-4ea0-abc9-aa091ce534fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# label\n",
    "df1 = df1.withColumn(\n",
    "    \"total_amount\",\n",
    "    col(\"base_passenger_fare\")\n",
    "  + col(\"tolls\")\n",
    "  + col(\"bcf\")\n",
    "  + col(\"sales_tax\")\n",
    "  + col(\"congestion_surcharge\")\n",
    "  + col(\"airport_fee\")\n",
    "  + col(\"tips\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08e6a-f3a9-4e2f-bf4f-694632116a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Class for cyclic encoders\n",
    "class CyclicalEncoder(Transformer):\n",
    "    def __init__(self, inputCol, period):\n",
    "        super().__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.period = period\n",
    "\n",
    "    def _transform(self, df):\n",
    "        angle = 2 * math.pi * F.col(self.inputCol) / float(self.period)\n",
    "        return (\n",
    "            df\n",
    "            .withColumn(f\"{self.inputCol}_sin\", F.sin(angle).cast(DoubleType()))\n",
    "            .withColumn(f\"{self.inputCol}_cos\", F.cos(angle).cast(DoubleType()))\n",
    "        )\n",
    "\n",
    "# 4) Create instances of cyclic encoders\n",
    "encoder_month = CyclicalEncoder(\"req_month\", period=12)\n",
    "encoder_day   = CyclicalEncoder(\"req_day\",   period=31)\n",
    "encoder_hour  = CyclicalEncoder(\"req_hour\",  period=24)\n",
    "encoder_min   = CyclicalEncoder(\"req_min\",   period=60)\n",
    "encoder_sec   = CyclicalEncoder(\"req_sec\",   period=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951a3ab-2618-43a5-8a09-7f1413343b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) StringIndexer for all categorical attributes ---\n",
    "\n",
    "# flags from fact\n",
    "idx_shared = StringIndexer(\n",
    "    inputCol=\"shared_match_flag\",\n",
    "    outputCol=\"shared_idx\"\n",
    ").setHandleInvalid(\"keep\")\n",
    "idx_access     = StringIndexer(inputCol=\"access_a_ride_flag\",   outputCol=\"access_idx\").setHandleInvalid(\"keep\")\n",
    "idx_wav_req    = StringIndexer(inputCol=\"wav_request_flag\",     outputCol=\"wav_req_idx\").setHandleInvalid(\"keep\")\n",
    "idx_wav_match  = StringIndexer(inputCol=\"wav_match_flag\",       outputCol=\"wav_match_idx\").setHandleInvalid(\"keep\")\n",
    "idx_request  = StringIndexer(inputCol=\"shared_request_flag\",       outputCol=\"request_idx\").setHandleInvalid(\"keep\")\n",
    "\n",
    "# base-dispatch\n",
    "idx_base      = StringIndexer(inputCol=\"dispatching_base_num\", outputCol=\"base_idx\").setHandleInvalid(\"keep\")\n",
    "\n",
    "# pickup-location\n",
    "idx_pu_zone         = StringIndexer(inputCol=\"zone\",            outputCol=\"pu_zone_idx\").setHandleInvalid(\"keep\")         # fence area\n",
    "idx_pu_borough      = StringIndexer(inputCol=\"borough\",         outputCol=\"pu_borough_idx\").setHandleInvalid(\"keep\")      # fence area\n",
    "idx_pu_service_zone = StringIndexer(inputCol=\"service_zone\",    outputCol=\"pu_service_zone_idx\").setHandleInvalid(\"keep\") # service area fence\n",
    "\n",
    "# dropoff-location\n",
    "idx_do_zone         = StringIndexer(inputCol=\"do_zone\",         outputCol=\"do_zone_idx\").setHandleInvalid(\"keep\")         \n",
    "idx_do_borough      = StringIndexer(inputCol=\"do_borough\",      outputCol=\"do_borough_idx\").setHandleInvalid(\"keep\")      \n",
    "idx_do_service_zone = StringIndexer(inputCol=\"do_service_zone\", outputCol=\"do_service_zone_idx\").setHandleInvalid(\"keep\")\n",
    "\n",
    "# --- 6) OneHotEncoder for all indexed categories ---\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols=[\n",
    "        \"shared_idx\", \"access_idx\", \"wav_req_idx\", \"wav_match_idx\", \"request_idx\",\n",
    "        \"base_idx\",\n",
    "        \"pu_zone_idx\", \"pu_borough_idx\", \"pu_service_zone_idx\",\n",
    "        \"do_zone_idx\", \"do_borough_idx\", \"do_service_zone_idx\"\n",
    "    ],\n",
    "    outputCols=[\n",
    "        \"shared_ohe\", \"access_ohe\", \"wav_req_ohe\", \"wav_match_ohe\", \"request_ohe\",\n",
    "        \"base_ohe\",\n",
    "        \"pu_zone_ohe\", \"pu_borough_ohe\", \"pu_service_zone_ohe\",\n",
    "        \"do_zone_ohe\", \"do_borough_ohe\", \"do_service_zone_ohe\"\n",
    "    ]\n",
    ").setHandleInvalid(\"keep\")\n",
    "\n",
    "# cyclic coding of the day of the week\n",
    "enc_day_of_week = CyclicalEncoder(\"day_of_week\", period=7)\n",
    "\n",
    "# indexer and OHE for the quarter\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "idx_quarter  = StringIndexer(inputCol=\"quarter\", outputCol=\"quarter_idx\")\n",
    "ohe_quarter  = OneHotEncoder(\n",
    "    inputCols=[\"quarter_idx\"],\n",
    "    outputCols=[\"quarter_ohe\"]\n",
    ").setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34208d64-eeb3-41a2-b0c1-d1081c7c8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Which numerical signs we take “as is” (now with month and index_level_0)\n",
    "numeric_feats = [\n",
    "    \"trip_miles\", \"trip_time\",\n",
    "    \"base_passenger_fare\", \"tolls\", \"bcf\", \"sales_tax\",\n",
    "    \"congestion_surcharge\", \"airport_fee\", \"tips\", \"driver_pay\",\n",
    "    \"month\", \"index_level_0\", \"total_amount\"\n",
    "]\n",
    "\n",
    "numeric_feats += [\n",
    "  \"req_year\"\n",
    "]\n",
    "\n",
    "numeric_feats.append(\"is_weekend_int\")\n",
    "\n",
    "# 8) Collect all features into a single vector (before the scaler)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=(\n",
    "        # ── cyclic sines/cosines for req_month, req_day, req_hour, req_min, req_sec ────\n",
    "        [\"req_month_sin\",\"req_month_cos\",\n",
    "         \"req_day_sin\",\"req_day_cos\",\n",
    "         \"req_hour_sin\",\"req_hour_cos\",\n",
    "         \"req_min_sin\",\"req_min_cos\",\n",
    "         \"req_sec_sin\",\"req_sec_cos\"]\n",
    "      + [\"shared_ohe\",\"access_ohe\",\"wav_req_ohe\",\"wav_match_ohe\",\"request_ohe\",\n",
    "         \"base_ohe\",\n",
    "         \"pu_zone_ohe\",\"pu_borough_ohe\",\"pu_service_zone_ohe\",\n",
    "         \"do_zone_ohe\",\"do_borough_ohe\",\"do_service_zone_ohe\"]\n",
    "        # + all numerical “as is”\n",
    "      + numeric_feats\n",
    "    ),\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# 9) StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0344d21-cb6f-44bd-aa62-f2df0ec08fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Transformer to fill null in categorical (fillna)\n",
    "class CategoricalImputer(Transformer):\n",
    "    \"\"\"\n",
    "    Fills in the gaps in the given row columns with the string “missing”.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputCols: list):\n",
    "        super().__init__()\n",
    "        self.inputCols = inputCols\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.fillna({col: \"missing\" for col in self.inputCols})\n",
    "\n",
    "# Imputer numeric\n",
    "num_imputer = Imputer(\n",
    "    inputCols=numeric_feats,           \n",
    "    outputCols=[f\"{c}_imputed\" for c in numeric_feats]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "# After that, you must replace the named columns themselves in numeric_feats:\n",
    "numeric_feats = [f\"{c}_imputed\" for c in numeric_feats]\n",
    "\n",
    "cat_columns = [\n",
    "    \"shared_match_flag\",\"access_a_ride_flag\",\"wav_request_flag\", \"shared_request_flag\",\n",
    "    \"wav_match_flag\",\"dispatching_base_num\",\n",
    "    \"zone\",\"borough\",\"service_zone\",\n",
    "    \"do_zone\",\"do_borough\",\"do_service_zone\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502d21f-d36a-4bc6-be2a-7dcb7c2155eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Estimator, Transformer\n",
    "from pyspark.ml.param.shared import Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# 10) PCA \n",
    "pca = PCA(\n",
    "    k=20,                       # выбери число компонент по своему усмотрению\n",
    "    inputCol=\"features\",        # вход — уже стандартизованные фичи\n",
    "    outputCol=\"pca_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb6b3f-7160-4fc6-a2ed-18ec844aea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [\n",
    "    CategoricalImputer(inputCols=cat_columns),  \n",
    "    num_imputer,  \n",
    "    # 1) cyclic encoders\n",
    "    encoder_month, encoder_day, encoder_hour, encoder_min, encoder_sec,\n",
    "    enc_day_of_week,\n",
    "    \n",
    "    # 2) indexers for categories\n",
    "    idx_shared, idx_access, idx_wav_req, idx_wav_match, idx_request,\n",
    "    idx_base,\n",
    "    idx_pu_zone, idx_pu_borough, idx_pu_service_zone,\n",
    "    idx_do_zone, idx_do_borough, idx_do_service_zone,\n",
    "    idx_quarter,\n",
    "    \n",
    "    # 3) one-hot \n",
    "    ohe,\n",
    "    ohe_quarter,\n",
    "    \n",
    "    # 4) assembly of all features into a vector\n",
    "    assembler,\n",
    "\n",
    "    # 5) scaler\n",
    "    scaler,\n",
    "    pca\n",
    "]\n",
    "\n",
    "# Pipeline\n",
    "preproc_pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de9462-b2da-410e-b33d-03fd99c0df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_sample = df1.sample(withReplacement=False, fraction=1/6, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f6677-961d-4d85-a2bd-ed559cd95e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "model = preproc_pipeline.fit(df1_sample)\n",
    "df_transformed = model.transform(df1_sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c67902-fa4a-472a-b16b-a9cbc50a9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml = (\n",
    "    df_transformed\n",
    "      .select(\n",
    "         col(\"features\").alias(\"features\"),\n",
    "         col(\"total_amount_imputed\").alias(\"label\")\n",
    "      )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a04540a3-5423-44f1-b1e9-e883328e4058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 17:11:46 WARN SQLConf: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0 and may be removed in the future. This config is replaced by 'spark.files.ignoreMissingFiles'.\n",
      "[Stage 110:====================================================>(383 + 1) / 384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1200813, Test: 800418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Split\n",
    "train_data, test_data = (\n",
    "    sampled \n",
    "    .randomSplit([0.6, 0.4], seed=42)\n",
    ")\n",
    "train_data = train_data.cache()\n",
    "\n",
    "# Sizes\n",
    "print(f\"Train: {train_data.count()}, Test: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3380b12b-7457-42b3-ad75-b350615be9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 18:02:26 WARN SQLConf: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0 and may be removed in the future. This config is replaced by 'spark.files.ignoreMissingFiles'.\n"
     ]
    }
   ],
   "source": [
    "sampled = data_ml.limit(100)\n",
    "\n",
    "# Split\n",
    "train_data, test_data = (\n",
    "    sampled \n",
    "    .randomSplit([0.6, 0.4], seed=42)\n",
    ")\n",
    "train_data = train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "873e4959-d5c2-43a1-8892-77b1ffabff83",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o14873.cache.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:278)\n\tat org.apache.spark.sql.SparkSession$.getOrCloneSessionWithConfigsOff(SparkSession.scala:1255)\n\tat org.apache.spark.sql.execution.CacheManager.getOrCloneSessionWithConfigsOff(CacheManager.scala:406)\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:121)\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)\n\tat org.apache.spark.sql.Dataset.persist(Dataset.scala:3775)\n\tat org.apache.spark.sql.Dataset.cache(Dataset.scala:3785)\n\tat sun.reflect.GeneratedMethodAccessor196.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 1.3 Cross-validation\u001b[39;00m\n\u001b[32m     21\u001b[39m cv_lr = CrossValidator(estimator=lr,\n\u001b[32m     22\u001b[39m                        estimatorParamMaps=paramGrid_lr,\n\u001b[32m     23\u001b[39m                        evaluator=evaluator_rmse,\n\u001b[32m     24\u001b[39m                        numFolds=\u001b[32m3\u001b[39m,\n\u001b[32m     25\u001b[39m                        parallelism=\u001b[32m8\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m cvModel_lr = \u001b[43mcv_lr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m best_lr    = cvModel_lr.bestModel\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 1.4 Сохранение модели\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/tuning.py:840\u001b[39m, in \u001b[36mCrossValidator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    838\u001b[39m datasets = \u001b[38;5;28mself\u001b[39m._kFold(dataset)\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFolds):\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m     validation = \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m     train = datasets[i][\u001b[32m0\u001b[39m].cache()\n\u001b[32m    843\u001b[39m     tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    844\u001b[39m         inheritable_thread_target,\n\u001b[32m    845\u001b[39m         _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    846\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1522\u001b[39m, in \u001b[36mDataFrame.cache\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK_DESER`).\u001b[39;00m\n\u001b[32m   1495\u001b[39m \n\u001b[32m   1496\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1519\u001b[39m \u001b[33;03m+- InMemoryTableScan ...\u001b[39;00m\n\u001b[32m   1520\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1521\u001b[39m \u001b[38;5;28mself\u001b[39m.is_cached = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1522\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o14873.cache.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:278)\n\tat org.apache.spark.sql.SparkSession$.getOrCloneSessionWithConfigsOff(SparkSession.scala:1255)\n\tat org.apache.spark.sql.execution.CacheManager.getOrCloneSessionWithConfigsOff(CacheManager.scala:406)\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:121)\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)\n\tat org.apache.spark.sql.Dataset.persist(Dataset.scala:3775)\n\tat org.apache.spark.sql.Dataset.cache(Dataset.scala:3785)\n\tat sun.reflect.GeneratedMethodAccessor196.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import os\n",
    "\n",
    "def run(cmd): return os.popen(cmd).read()\n",
    "\n",
    "# 1.1 Настройка LR и evaluators\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# 1.2 Параметрическая сетка\n",
    "paramGrid_lr = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,       [0.01, 0.1])\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# 1.3 Cross-validation\n",
    "cv_lr = CrossValidator(estimator=lr,\n",
    "                       estimatorParamMaps=paramGrid_lr,\n",
    "                       evaluator=evaluator_rmse,\n",
    "                       numFolds=3,\n",
    "                       parallelism=8)\n",
    "\n",
    "cvModel_lr = cv_lr.fit(train_data)\n",
    "best_lr    = cvModel_lr.bestModel\n",
    "\n",
    "# 1.4 Сохранение модели\n",
    "best_lr.write().overwrite().save(\"home/team6/project/models/model1\")\n",
    "run(\"mkdir -p ../models\")\n",
    "run(\"hdfs dfs -get /user/team6/project/models/model1 ../models/model1\")\n",
    "\n",
    "# 1.5 Предсказания и сохранение\n",
    "pred_lr = best_lr.transform(test_data)\n",
    "pred_lr.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model1_predictions.csv\")\n",
    "run(\"mkdir -p ../output\")\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions.csv/*.csv > ../output/model1_predictions.csv\")\n",
    "\n",
    "# 1.6 Метрики\n",
    "rmse_lr = evaluator_rmse.evaluate(pred_lr)\n",
    "r2_lr   = evaluator_r2.evaluate(pred_lr)\n",
    "print(f\"LR RMSE = {rmse_lr:.4f}, R2 = {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67d915-5626-46bf-b629-3f502333833a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
